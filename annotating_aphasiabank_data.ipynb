{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c43ad1c-2bd3-4d52-861e-79d94111d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114d87f-a46e-44b1-be18-d1c4d72cfa94",
   "metadata": {},
   "source": [
    "### Importing preprocessed file from preprocessing_aphasiabank.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09dc951e-dfb6-4535-91db-59b0dae664cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(\"all_processed_files.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61a10d6-ad98-4c33-94d4-814be07b60f1",
   "metadata": {},
   "source": [
    "### Annotating with general information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41f9e54c-af5f-4745-bc0f-3aac6adfd51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4905\n"
     ]
    }
   ],
   "source": [
    "df = df_original[[\"Person\", \"Text\", \"mor\", \"gra\"]][:5000].copy()\n",
    "df = df.dropna()\n",
    "print(len(df))\n",
    "\n",
    "def get_duration(sentence):\n",
    "    try:\n",
    "        return (re.search(\".+ \\\\x15(.+)\\\\x15\", sentence).group(1))\n",
    "    except AttributeError:\n",
    "         return \"NaN\"\n",
    "        \n",
    "def remove_time(sentence):\n",
    "    try:\n",
    "        return sentence.replace(re.search(\"\\\\x15.+\", sentence).group(0), \"\")\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "def duration_to_seconds(sentence):\n",
    "    try:\n",
    "        return float((int(re.search(\"_.+\", sentence).group(0)[1:]) - int(re.search(\".+_\", sentence).group(0)[:-1]))/1000)\n",
    "    except AttributeError:\n",
    "        return \"NaN\"\n",
    "    \n",
    "def assign_hesitation(text):\n",
    "    sentence = text\n",
    "    return (re.findall(\"(&-.+?) \", sentence))\n",
    "\n",
    "def return_clean_text(sentence, steps = False):\n",
    "    removed_gesture = re.sub(\"&=.+? \", \"\", str(sentence))#remove gestures first\n",
    "    removed_correction_marker = re.sub(\"\\[.+?\\] \", \"\", removed_gesture)\n",
    "    removed_hesitations = re.sub(\"&-\", \"\", removed_correction_marker)\n",
    "    only_words = re.sub(\"[^\\w ']\", \"\", removed_hesitations)# remove non-characters except for '\n",
    "    if steps == False:\n",
    "        #print(only_words)\n",
    "        return only_words\n",
    "    if steps == True:\n",
    "        #with f' you can't use \"\\n\"\n",
    "        print (\"0, Original sentence:\", sentence,\n",
    "                \"1, Removed gestures:\", removed_gesture,\n",
    "                \"2, Removed corrections:\", removed_correction_marker,\n",
    "                \"3, Removed hesitations:\", removed_hesitations,\n",
    "                \"4, Only words: \", only_words, sep=\"\\n\")\n",
    "        print(\"<------------------------------------------------------------------------------>\")\n",
    "        return only_words\n",
    "\n",
    "def retrieve_corrections(sentence):\n",
    "    corrections = []\n",
    "    first_layer = re.findall(\"(?<=\\s).+? \\[:.+?]\", sentence)\n",
    "    if len(first_layer) != 0:\n",
    "        for i in first_layer:\n",
    "            if re.search(\"\\[: (.+?)]\", i).group(1) == \"x@n\":\n",
    "                corrections.append(\"unintelligible\")\n",
    "            else:\n",
    "                second_layer = re.sub(\"\\[: \", \"\", i)\n",
    "                third_layer = re.sub(\"\\]\", \"\", second_layer)\n",
    "                fourth_layer = re.sub(\"<\", \"\", third_layer)\n",
    "                fifth_layer = re.sub(\"@u\", \"\", fourth_layer)\n",
    "                sixth_layer = fifth_layer.split()\n",
    "                corrections.append(tuple([sixth_layer[-2],sixth_layer[-1]]))\n",
    "    else:\n",
    "        corrections = \"NaN\"\n",
    "    return corrections\n",
    "\n",
    "df[\"hesitations\"] = df[\"Text\"].apply(lambda x: assign_hesitation(x))\n",
    "df[\"corrections\"] = df[\"Text\"].apply(lambda x: retrieve_corrections(x))\n",
    "df[\"duration\"] = df[\"Text\"].apply(lambda x: get_duration(x))\n",
    "df[\"Text\"] = df[\"Text\"].apply(lambda x: remove_time(x))\n",
    "df[\"seconds\"] = df[\"duration\"].apply(lambda x: duration_to_seconds(x))\n",
    "df[\"Clean_text\"] = df[\"Text\"].apply(lambda x: return_clean_text(x, False))\n",
    "df[\"num_corrections\"] = df[\"corrections\"].apply(lambda x: len(x))\n",
    "df[\"num_unintelligible\"] = df[\"corrections\"].apply(lambda x: len([z for z in x if z == \"unintelligible\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07524be5-22b1-4ac8-9256-3863b3af2434",
   "metadata": {},
   "source": [
    "### Annotating with linguistic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "320f981c-dfbb-4c75-b2d3-14890fcf7731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"xcomp\"] = df[\"gra\"].apply(lambda x: 1 if \"XCOMP\" in x else 0)\n",
    "df[\"ccomp\"] = df[\"gra\"].apply(lambda x: 1 if \"COMP\" in x else 0)\n",
    "df[\"prepositional\"] = df[\"gra\"].apply(lambda x: 1 if \"LOC\" in x else 0)\n",
    "df[\"optional_elements\"] = df[\"gra\"].apply(lambda x: 1 if (\"JCT\" or \"CJCT\" or \"XJCT\") in x else 0)\n",
    "df[\"noun_modifiers\"] = df[\"gra\"].apply(lambda x: 1 if (\"MOD\" or \"CMOD\" or \"XMOD\") in x else 0)\n",
    "df[\"negation\"] = df[\"gra\"].apply(lambda x: 1 if \"NEG\" in x else 0)\n",
    "df[\"determiner\"] = df[\"gra\"].apply(lambda x: 1 if \"DET\" in x else 0)\n",
    "df[\"topicalization\"] = df[\"gra\"].apply(lambda x: 1 if \"TOP\" in x else 0)\n",
    "df[\"quantifier\"] = df[\"gra\"].apply(lambda x: 1 if \"QUANT\" in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31f4060a-ad75-4af3-97f3-c9db90cb3bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Person',\n",
       " 'Text',\n",
       " 'mor',\n",
       " 'gra',\n",
       " 'hesitations',\n",
       " 'corrections',\n",
       " 'duration',\n",
       " 'seconds',\n",
       " 'Clean_text',\n",
       " 'num_corrections',\n",
       " 'num_unintelligible',\n",
       " 'xcomp',\n",
       " 'ccomp',\n",
       " 'prepositional',\n",
       " 'optional_elements',\n",
       " 'noun_modifiers',\n",
       " 'negation',\n",
       " 'determiner',\n",
       " 'topicalization',\n",
       " 'quantifier']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2501fabc-3a80-4d8e-82bd-6f62da6288b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person</th>\n",
       "      <th>Text</th>\n",
       "      <th>mor</th>\n",
       "      <th>gra</th>\n",
       "      <th>hesitations</th>\n",
       "      <th>corrections</th>\n",
       "      <th>duration</th>\n",
       "      <th>seconds</th>\n",
       "      <th>Clean_text</th>\n",
       "      <th>num_corrections</th>\n",
       "      <th>num_unintelligible</th>\n",
       "      <th>xcomp</th>\n",
       "      <th>ccomp</th>\n",
       "      <th>prepositional</th>\n",
       "      <th>optional_elements</th>\n",
       "      <th>noun_modifiers</th>\n",
       "      <th>negation</th>\n",
       "      <th>determiner</th>\n",
       "      <th>topicalization</th>\n",
       "      <th>quantifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*INV:</td>\n",
       "      <td>so ‡ first we're just gonna do some talking .</td>\n",
       "      <td>co|so beg|beg adv|first pro:sub|we~aux|be&amp;PRES...</td>\n",
       "      <td>1|0|BEG 2|1|BEGP 3|7|JCT 4|7|SUBJ 5|7|AUX 6|7|...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0_5819</td>\n",
       "      <td>5.819</td>\n",
       "      <td>so  first we're just gonna do some talking</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*PAR:</td>\n",
       "      <td>+&lt; okay .</td>\n",
       "      <td>co|okay .</td>\n",
       "      <td>1|0|INCROOT 2|1|PUNCT</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5119_5778</td>\n",
       "      <td>0.659</td>\n",
       "      <td>okay</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*INV:</td>\n",
       "      <td>so ‡ okay .</td>\n",
       "      <td>co|so beg|beg co|okay .</td>\n",
       "      <td>1|0|BEG 2|1|BEGP 3|0|INCROOT 4|3|PUNCT</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5819_10856</td>\n",
       "      <td>5.037</td>\n",
       "      <td>so  okay</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*INV:</td>\n",
       "      <td>how do you think your speech is these days ?</td>\n",
       "      <td>pro:int|how mod|do pro:per|you v|think det:pos...</td>\n",
       "      <td>1|4|JCT 2|4|AUX 3|4|SUBJ 4|0|ROOT 5|6|DET 6|7|...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10856_15376</td>\n",
       "      <td>4.52</td>\n",
       "      <td>how do you think your speech is these days</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*PAR:</td>\n",
       "      <td>alright &amp;=ges . [+ gram]</td>\n",
       "      <td>co|alright .</td>\n",
       "      <td>1|0|INCROOT 2|1|PUNCT</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15376_16794</td>\n",
       "      <td>1.418</td>\n",
       "      <td>alright</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Person                                            Text  \\\n",
       "0  *INV:  so ‡ first we're just gonna do some talking .    \n",
       "1  *PAR:                                      +< okay .    \n",
       "2  *INV:                                    so ‡ okay .    \n",
       "3  *INV:   how do you think your speech is these days ?    \n",
       "4  *PAR:                       alright &=ges . [+ gram]    \n",
       "\n",
       "                                                 mor  \\\n",
       "0  co|so beg|beg adv|first pro:sub|we~aux|be&PRES...   \n",
       "1                                          co|okay .   \n",
       "2                            co|so beg|beg co|okay .   \n",
       "3  pro:int|how mod|do pro:per|you v|think det:pos...   \n",
       "4                                       co|alright .   \n",
       "\n",
       "                                                 gra hesitations corrections  \\\n",
       "0  1|0|BEG 2|1|BEGP 3|7|JCT 4|7|SUBJ 5|7|AUX 6|7|...          []         NaN   \n",
       "1                              1|0|INCROOT 2|1|PUNCT          []         NaN   \n",
       "2             1|0|BEG 2|1|BEGP 3|0|INCROOT 4|3|PUNCT          []         NaN   \n",
       "3  1|4|JCT 2|4|AUX 3|4|SUBJ 4|0|ROOT 5|6|DET 6|7|...          []         NaN   \n",
       "4                              1|0|INCROOT 2|1|PUNCT          []         NaN   \n",
       "\n",
       "      duration seconds                                    Clean_text  \\\n",
       "0       0_5819   5.819  so  first we're just gonna do some talking     \n",
       "1    5119_5778   0.659                                        okay     \n",
       "2   5819_10856   5.037                                    so  okay     \n",
       "3  10856_15376    4.52  how do you think your speech is these days     \n",
       "4  15376_16794   1.418                                     alright     \n",
       "\n",
       "   num_corrections  num_unintelligible  xcomp  ccomp  prepositional  \\\n",
       "0                3                   0      0      1              0   \n",
       "1                3                   0      0      0              0   \n",
       "2                3                   0      0      0              0   \n",
       "3                3                   0      0      1              0   \n",
       "4                3                   0      0      0              0   \n",
       "\n",
       "   optional_elements  noun_modifiers  negation  determiner  topicalization  \\\n",
       "0                  1               0         0           0               0   \n",
       "1                  0               0         0           0               0   \n",
       "2                  0               0         0           0               0   \n",
       "3                  1               0         0           1               0   \n",
       "4                  0               0         0           0               0   \n",
       "\n",
       "   quantifier  \n",
       "0           1  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6bf45-7b25-4eab-a2bd-1df30ebb5ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
